{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import gzip\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import pystan\n",
    "import lda\n",
    "import lda.datasets\n",
    "import matplotlib.cm as cm\n",
    "import cPickle as cpkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loading the preprocessed data. See data_processing.ipynb for details\n",
    "with open('preprocessed_data.cpkl','r') as f:\n",
    "    data = cpkl.load(f)\n",
    "    \n",
    "x_train = data['x_train']     #one hot matrix (samples x genes)\n",
    "t_train = data['t_train']     #labels from bioinformatician \n",
    "genes_id = data['genes_id']   \n",
    "genes = data['genes']\n",
    "sampleid = data['sampleid']   #sampleid and words contain the same information as x_train just encoded as sampleid: cell, words: id for seen words  \n",
    "words = data['words']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "LDA_uniprior = \"\"\"\n",
    "\n",
    "data {\n",
    "  int<lower=2> K;               // num topics\n",
    "  int<lower=2> V;               // num words\n",
    "  int<lower=1> M;               // num docs\n",
    "  int<lower=1> N;               // total word instances\n",
    "  int<lower=1,upper=V> w[N];    // word n\n",
    "  int<lower=1,upper=M> doc[N];  // doc ID for word n\n",
    "  vector<lower=0>[K] alpha;     // topic prior\n",
    "  vector<lower=0>[V] beta;      // word prior\n",
    "}\n",
    "parameters {\n",
    "  simplex[K] theta[M];   // topic dist for doc m\n",
    "  simplex[V] phi[K];     // word dist for topic k\n",
    "}\n",
    "model {\n",
    "  for (m in 1:M)  \n",
    "    theta[m] ~ dirichlet(alpha);  // prior\n",
    "  for (k in 1:K)  \n",
    "    phi[k] ~ dirichlet(beta);     // prior\n",
    "  for (n in 1:N) {\n",
    "    real gamma[K];\n",
    "    for (k in 1:K) \n",
    "      gamma[k] <- log(theta[doc[n],k]) + log(phi[k,w[n]]);\n",
    "    increment_log_prob(log_sum_exp(gamma));  // likelihood\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "M,V  = x_train.shape\n",
    "K=5\n",
    "V=48\n",
    "M=1430\n",
    "data = dict(K=K,\n",
    "        V=V,\n",
    "        M=M,\n",
    "        N=np.sum(x_train),\n",
    "        w=words,\n",
    "        doc=sampleid,\n",
    "        alpha=np.ones(K,)*(1./K),\n",
    "        beta=np.ones(V,)*(1./V),\n",
    "           )\n",
    "\n",
    "fit_uni_prior = pystan.stan(model_code=LDA_uniprior, data=data)\n",
    "\n",
    "\n",
    "samples = fit_uni_prior.extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA_informative_prior = \"\"\"\n",
    "\n",
    "data {\n",
    "  int<lower=2> K;               // num topics\n",
    "  int<lower=2> V;               // num words\n",
    "  int<lower=1> M;               // num docs\n",
    "  int<lower=1> N;               // total word instances\n",
    "  int<lower=1,upper=V> w[N];    // word n\n",
    "  int<lower=1,upper=M> doc[N];  // doc ID for word n\n",
    "  vector<lower=0>[K] alpha;     // topic prior\n",
    "  vector<lower=0>[V] beta;      // word prior\n",
    "}\n",
    "parameters {\n",
    "  simplex[K] theta[M];   // topic dist for doc m\n",
    "  simplex[V] phi[K];     // word dist for topic k\n",
    "}\n",
    "model {\n",
    "  for (m in 1:M)  \n",
    "    theta[m] ~ dirichlet(alpha);  // prior\n",
    "  for (k in 1:K)  \n",
    "    phi[k] ~ dirichlet(beta);     // prior\n",
    "  for (n in 1:N) {\n",
    "    real gamma[K];\n",
    "    for (k in 1:K) \n",
    "      gamma[k] <- log(theta[doc[n],k]) + log(phi[k,w[n]]);\n",
    "    increment_log_prob(log_sum_exp(gamma));  // likelihood\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "M,V  = x_train.shape\n",
    "K=5\n",
    "V=48\n",
    "M=1430\n",
    "data = dict(K=K,\n",
    "        V=V,\n",
    "        M=M,\n",
    "        N=np.sum(x_train),\n",
    "        w=words,\n",
    "        doc=sampleid,\n",
    "        alpha=np.ones(K,)*(1./K),\n",
    "        beta=(1+x_train.sum(axis=1).astype('float32'))/x_train.sum(),\n",
    "           )\n",
    "\n",
    "fit_infor_prior = pystan.stan(model_code=LDA_informative_prior, data=data)\n",
    "\n",
    "fit_infor_prior.extract()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
